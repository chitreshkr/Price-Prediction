---
title: "Homework2_BUAN6356503_Group10"
author: ("Chitresh Kumar","Chaitanya Narella","Disha Punjabi","Nidaa Tamkeen","Mai Han Tran")
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:\\Users\\chitr\\OneDrive\\Documents")
getwd()
#Loading the required packages to R
pacman::p_load(tidyverse, reshape, gplots, ggmap,dplyr, 
               mlbench,caret, data.table, e1071, fpp2, gains, pROC,leaps,
forecast,  GGally,scales, mosaic, mapproj, mlbench, data.table,broom,caret,DBI,dbplyr,forecast,ggplot2,knitr,latexpdf,leaps,lmtest,reshape2,yaml,zealot,gridExtra,ggpubr,rpivotTable)
#data.table, forecast, leaps, tidyverse,gplots, ggplot2, ggpubr, gridExtra,rpivotTable,dplyr

```


## R Markdown
**Question 1)Create a correlation table and scatterplots between FARE and the predictors. What seems to be the best single predictor of FARE? Explain your answer.**
```{r Airfare Data}
airfare.df <- read.csv("Airfares.csv")
air.df <- airfare.df[,-c(1,2,3,4)]
data_airfares <- air.df[-c(3,4,10,11)]
air.dt <- setDT(air.df)
```

## Including Plots


```{r Correlation}
cor.mat <- round(cor(air.dt[,!c("S_CODE","S_CITY","E_CODE","E_CITY","VACATION","SW","SLOT","GATE")]),2)
cor.mat
melted.cor.mat <- melt(cor.mat) 
melted.cor.mat
ggplot(melted.cor.mat, aes(x = Var1, y = Var2, fill = value)) + 
  scale_fill_gradient(low="wheat", high="orangered") +
  geom_tile() + 
  geom_text(aes(x = Var1, y = Var2, label = value))



```

```Coupon and distance have strong positive correlation .As distance increases coupon increases.```
```Fare and distance has positive correlation .As distance increases the fare will increase```
```Fare and coupon has positive correlation .As coupon increases the fare will increase```
```{r Box Plots}
plot_1 <- ggplot(data_airfares ) +
geom_point(aes(x= NEW, y = FARE ), size = 1,colour="blue") + ggtitle("New Flights vs Fare")

plot_2 <- ggplot(data_airfares )+
geom_point(aes(x= COUPON, y = FARE ), size = 1,colour="blue") + ggtitle("Coupon vs Fare")


plot_3 <- ggplot(data_airfares )+
geom_point(aes(x= HI, y = FARE ), size = 1,colour="blue")+ ggtitle("HI vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_4 <- ggplot(data_airfares )+
geom_point(aes(x= S_INCOME, y = FARE ), size = 1,colour="blue")+ ggtitle("S_Income vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_5 <- ggplot(data_airfares )+
geom_point(aes(x= E_INCOME, y = FARE ), size = 1,colour="blue")+ ggtitle("E_Income vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_6 <- ggplot(data_airfares )+
geom_point(aes(x= S_POP, y = FARE ), size = 1,colour="blue")+ ggtitle("Start_City_Population vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_7 <- ggplot(data_airfares )+
geom_point(aes(x= E_POP, y = FARE ), size = 1,colour="blue")+ ggtitle("End_City_Population vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_8 <- ggplot(data_airfares )+
geom_point(aes(x= DISTANCE, y = FARE ), size = 1,colour="blue")+ggtitle("Distance vs Fare")+
theme(axis.text.x = element_text(angle = 90))

plot_9 <- ggplot(data_airfares )+
geom_point(aes(x= PAX, y = FARE ), size = 1,colour="blue")+ ggtitle("Pax vs Fare")+
theme(axis.text.x = element_text(angle = 90))

grid.arrange(plot_1, plot_2, plot_3, plot_4, plot_5, plot_6, plot_7, plot_8,
 plot_9, nrow = 3)
```
```Evidently from the scatterplot and data, Distance is the best single predictor of Fare. They both are highly correlated as compared to the other predictors. The scatterplot reflects strong positive correlation between Distance and Fare.```

```Question 2)Explore the categorical predictors by computing the percentage of flights in each category. Create a pivot table with the average fare in each category. Which categorical predictor seems best for predicting FARE? Explain your answer.```
```{r Pivot Table}
#View the pivot table of Vacation
Vacation <- air.df %>%
dplyr::select(VACATION,FARE) %>%
group_by(VACATION) %>%
summarise(Count = length(VACATION),Total = nrow(air.df),
Percent = (length(VACATION)/nrow(air.df)) *100 ,
AvgFare = mean(FARE))

Vacation

#View the pivot table of SouthWest
Southwest <- air.df %>%
dplyr::select(SW,FARE) %>%
group_by(SW) %>%
summarise(Count = length(SW),Total = nrow(air.df),
Percent = (length(SW)/nrow(air.df))* 100,
AvgFare = mean(FARE))
Southwest

#View the pivot table of Gate
Gate <- air.df %>%
dplyr::select(GATE,FARE) %>%
group_by(GATE) %>%
summarise(Count = length(GATE),Total = nrow(air.df),
Percent = (length(GATE)/nrow(air.df))*100,
AvgFare = mean(FARE))
Gate


Slot <- air.df %>%
dplyr::select(SLOT,FARE) %>%
group_by(SLOT) %>%
summarise(Count = length(SLOT),Total = nrow(air.df),
Percent = (length(SLOT)/nrow(air.df))*100,
AvgFare = mean(FARE))
Slot

```
```
From the above scenario, Southwest Airline is a highly impacting categorical predictor. It strikingly affects the average fare and therefore is best for predicting fare

Question 3) Create data partition by assigning 80% of the records to the training dataset. Use rounding if 80% of the index generates a fraction. Also, set the seed at 42
```
**Linear Regression Model**
```{r Regression}
a <- nrow(air.df)
b <- a*0.80;
round(b,digits=0)

set.seed(42)  
train.index <- sample(c(1:510), 128)  
train.df <- air.df[-train.index, ]
valid.df <- air.df[+train.index, ]

air.lm <- lm(FARE~ ., data = train.df)

options(scipen = 999)
summary(air.lm)

class(air.lm)
methods(class=class(air.lm))
confint(air.lm)

par(mfrow = c(2,2))
plot(air.lm)
par(mfrow = c(10,10))

air.lm.pred <-predict(air.lm,valid.df)

some.residuals <- valid.df$FARE[1:128] - air.lm.pred[1:128]


data.frame("Predicted" = air.lm.pred[1:128], "Actual" = valid.df$FARE[1:128],
           "Residual" = some.residuals)
accuracy(air.lm.pred,valid.df$FARE)
```
```From the probibility values we can figure out Vacation,Herfindahl index,SouthWest Airline serving the route ,Destination Population Income,Starting city’s population,End, city’s population,SlotFree,GateFree,Distance,Number of passengers are significant variables for linear regression modelling```

```From Residual VS Fitted Values We see the values are evenly distributed around the line so our linear assumption is valid``
```From Normal Q-Q We can see that only one outlier is not following the normal curve and every other values are following normal line so our assumption of Normal Distribution is valid too```
```From Scale Location Plot we see the values are evenly distributed above and below the residual line therefore we are satisfying homoskadacity```
```From Residuals Vs Leverage plot we can see that 373 is an influential observation and there would be some change in the model```


```Question4)Using leaps package, run stepwise regression to reduce the number of predictors.Discuss the results from this model```
**Stepwise Regression**

```{r Stepwise Regression}
search.stepwise <- regsubsets(FARE ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
                     method = "seqrep")
sum <- summary(search.stepwise)
sum$which
sum$rsq
sum$adjr2
sum$cp
```

```From squared R we are getting the highest value when we are considering all 13 variables.We need to consider 12 variables because  mallow cp is lowest for 12 variable model and highest adjusted r squared for 12 variable model```


```Question 5 Repeat the process in (4) using exhaustive search instead of stepwise regression. Compare the resulting best model to the one you obtained in (4) in terms of the predictors included in the final model.```
```{r Exhaustive Search}
search.exhaustive <- regsubsets(FARE ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
                     method = "exhaustive")
sum <- summary(search.exhaustive)


sum$which


sum$rsq
sum$adjr2
sum$cp
```
```From squared R we are getting the highest value when we are considering all 13 variables.We need to consider 12 variables because  mallow cp is lowest for 12 variable model and highest adjusted r squared for 12 variable model```


```{r}
# 6. Compare the predictive accuracy of both models—stepwise regression and exhaustive search—using measures such as RMSE.

air.lm.stepwise <- step(lm(FARE ~ ., data = train.df),direction="both")
stepwise.pred <- predict(air.lm.stepwise, valid.df)
accuracy(stepwise.pred, valid.df$FARE)

air.lm.exhaustive <- lm(FARE ~ NEW+VACATION+SW+HI+S_INCOME+E_INCOME+S_POP+E_POP+GATE+SLOT+DISTANCE+PAX,data = train.df)
exhaustive.pred<-predict(air.lm.exhaustive,valid.df)
accuracy(exhaustive.pred,valid.df$FARE)

```
 ```The RMSE value for Exhaustive Model is 31.03422 and the RMSE value for Stepwise Regression Model is 30.8338.Lesser RMSE value, the better the fit.Hence, we concclude by saying the Stepwise Regression Model is a slightly better fit than the Exhaustive Search model; although both models are similar since the RMSE values are comparable.```
#Keeping in mind the number of variables and the values of RMSE, Stepwise Regression Model is more attractive.


```Question 7) Using the exhaustive search model, predict the average fare on a route with the following characteristics: COUPON = 1.202, NEW = 3, VACATION = No, SW = No, HI = 4442.141, S_INCOME = $28,760, E_INCOME = $27,664, S_POP = 4,557,004, E_POP = 3,195,503, SLOT = Free, GATE = Free, PAX = 12,782, DISTANCE = 1976 miles```

```{r}
newrow <- list(COUPON = 1.202, NEW = 3, VACATION = "No", SW = "No", HI = 4442.141, S_INCOME = 28760, E_INCOME = 27664, S_POP = 4557004, E_POP = 3195503, SLOT = "Free", GATE = "Free", PAX = 12782, DISTANCE = 1976, FARE = 0)
new <- rbind(air.df, newrow)
newrow.df <- new[nrow(new),]
test_mat = model.matrix(FARE ~ ., data = newrow.df)
coefs = coef(search.exhaustive, id = 12)
prednew = test_mat[, names(coefs)] %*% coefs
prednew
```

```Question 8) Predict the reduction in average fare on the route in question  if Southwest decides to cover this route```

```{r}

newrow2 <- list(COUPON = 1.202, NEW = 3, VACATION = "No", SW = "Yes", HI = 4442.141, S_INCOME = 28760, E_INCOME = 27664, S_POP = 4557004, E_POP = 3195503, SLOT = "Free", GATE = "Free", PAX = 12782, DISTANCE = 1976, FARE = 0)
new2 <- rbind(new, newrow2)
newrow2.df <- new2[nrow(new2),]
test_mat2 = model.matrix(FARE ~ ., data = newrow2.df)
coefs = coef(search.exhaustive, id = 12)
prednew2 = test_mat2[, names(coefs)] %*% coefs
prednew2
```

```There is drop in the value of 45 in the fare if southwest airlines starts operating```


**Question 9 Using leaps package, run backward selection regression to reduce the number of predictors. Discuss the results from this model.**

```{r Backward elimination Using Leaps Package}
search <- regsubsets(FARE ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
                     method = "backward")
sum <- summary(search)

# show models
sum$which

# show metrics
sum$rsq
sum$adjr2
sum$cp
```
```The R squared value for the 13 varibles is the highest. Subsequently the adjusted R squared value for the 12 varibles is highest and also the Mallows Cp value also reflects the same for the 12 variables which is the lowest.The backward model has removed the COUPON variable.```


```Question 10)Now run a backward selection model using stepAIC() function. Discuss the results from this model, including the role of AIC in this model```
```{r STEPAIC backward}
library(MASS)
air.lm.stepwise <- stepAIC(air.lm, direction = "backward")
summary(air.lm.stepwise)  
air.lm.stepwise.pred <- predict(air.lm.stepwise, valid.df)
accuracy(air.lm.stepwise.pred, valid.df$FARE)

```
```The StepAIC model is based on the Akaike information Criteria .According to this model the Variblewith the lowest AIC values are removed and subsequently the results are produced .Here the lowest AIC value is for the COUNPON variable and is therefore removed .Considering the AIC value for all the 13 varibles is 3604.91 when the variable COUNPON has been removed the AIC value is 3603.41.```